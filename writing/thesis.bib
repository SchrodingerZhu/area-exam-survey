@book{bruegge2004object,
	title = {Object Oriented Software Engineering Using UML, Patterns, and Java},
	author = {Bruegge, Bernd and Dutoit, Allen H},
	year = {2009},
	publisher = {Prentice Hall}
}
@misc{terence_tao_lean_tour,
	title = {A slightly longer {Lean 4} proof tour},
	author = {Tao, Terence},
	url = {https://terrytao.wordpress.com/2023/12/05/a-slightly-longer-lean-4-proof-tour/},
	year = {2023}
}
@misc{Fermat_last_theorem,
	title = {{Fermat's Last Theorem} for regular primes},
	author = {LeanProver Community},
	url = {https://leanprover-community.github.io/flt-regular/blueprint/},
	year = {2024}
}
@article{Leroy-Compcert-CACM,
  author = {Xavier Leroy},
  title = {Formal verification of a realistic compiler},
  journal = {Communications of the ACM},
  year = 2009,
  volume = 52,
  number = 7,
  pages = {107--115},
  url = {http://xavierleroy.org/publi/compcert-CACM.pdf},
  urlpublisher = {http://doi.acm.org/10.1145/1538788.1538814},
  hal = {http://hal.archives-ouvertes.fr/inria-00415861/},
  pubkind = {journal-int-mono},
  abstract = {This paper reports on the development and formal verification (proof
of semantic preservation) of CompCert, a compiler from Clight (a
large subset of the C programming language) to PowerPC assembly code,
using the Coq proof assistant both for programming the compiler and
for proving its correctness.  Such a verified compiler is useful in
the context of critical software and its formal verification: the
verification of the compiler guarantees that the safety properties
proved on the source code hold for the executable compiled code as
well.}
}

@article{composable-verification,
author = {Zhang, Ling and Wang, Yuting and Wu, Jinhua and Koenig, J\'{e}r\'{e}mie and Shao, Zhong},
title = {Fully Composable and Adequate Verified Compilation with Direct Refinements between Open Modules},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632914},
doi = {10.1145/3632914},
abstract = {Verified compilation of open modules (i.e., modules whose functionality depends on other modules) provides a foundation for end-to-end verification of modular programs ubiquitous in contemporary software. However, despite intensive investigation in this topic for decades,the proposed approaches are still difficult to use in practice as they rely on assumptions about the internal working of compilers which make it difficult for external users to apply the verification results. We propose an approach to verified compositional compilation without such assumptions in the setting of verifying compilation of heterogeneous modules written in first-order languages supporting global memory and pointers. Our approach is based on the memory model of CompCert and a new discovery that a Kripke relation with a notion of memory protection can serve as a uniform and composable semantic interface for the compiler passes. By absorbing the rely-guarantee conditions on memory evolution for all compiler passes into this Kripke Memory Relation and by piggybacking requirements on compiler optimizations onto it, we get compositional correctness theorems for realistic optimizing compilers as refinements that directly relate native semantics of open modules and that are ignorant of intermediate compilation processes. Such direct refinements support all the compositionality and adequacy properties essential for verified compilation of open modules. We have applied this approach to the full compilation chain of CompCert with its Clight source language and demonstrated that our compiler correctness theorem is open to composition and intuitive to use with reduced verification complexity through end-to-end verification of non-trivial heterogeneous modules that may freely invoke each other (e.g.,mutually recursively).},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {72},
numpages = {31},
keywords = {Direct Refinements, Kripke Relations, Verified Compositional Compilation}
}
@article{10.1145/173262.155107,
author = {Grunwald, Dirk and Zorn, Benjamin and Henderson, Robert},
title = {Improving the cache locality of memory allocation},
year = {1993},
issue_date = {June 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/173262.155107},
doi = {10.1145/173262.155107},
abstract = {The allocation and disposal of memory is a ubiquitous operation in most programs. Rarely do programmers concern themselves with details of memory allocators; most assume that memory allocators provided by the system perform well. This paper presents a performance evaluation of the reference locality of dynamic storage allocation algorithms based on trace-driven simualtion of five large allocation-intensive C programs. In this paper, we show how the design of a memory allocator can significantly affect the reference locality for various applications. Our measurements show that poor locality in sequential-fit allocation algorithms reduces program performance, both by increasing paging and cache miss rates. While increased paging can be debilitating on any architecture, cache misses rates are also important for modern computer architectures. We show that algorithms attempting to be space-efficient by coalescing adjacent free objects show poor reference locality, possibly negating the benefits of space efficiency. At the other extreme, algorithms can expend considerable effort to increase reference locality yet gain little in total execution performance. Our measurements suggest an allocator design that is both very fast and has good locality of reference.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {177–186},
numpages = {10}
}

@inproceedings{locality-alloc,
author = {Grunwald, Dirk and Zorn, Benjamin and Henderson, Robert},
title = {Improving the cache locality of memory allocation},
year = {1993},
isbn = {0897915984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/155090.155107},
doi = {10.1145/155090.155107},
abstract = {The allocation and disposal of memory is a ubiquitous operation in most programs. Rarely do programmers concern themselves with details of memory allocators; most assume that memory allocators provided by the system perform well. This paper presents a performance evaluation of the reference locality of dynamic storage allocation algorithms based on trace-driven simualtion of five large allocation-intensive C programs. In this paper, we show how the design of a memory allocator can significantly affect the reference locality for various applications. Our measurements show that poor locality in sequential-fit allocation algorithms reduces program performance, both by increasing paging and cache miss rates. While increased paging can be debilitating on any architecture, cache misses rates are also important for modern computer architectures. We show that algorithms attempting to be space-efficient by coalescing adjacent free objects show poor reference locality, possibly negating the benefits of space efficiency. At the other extreme, algorithms can expend considerable effort to increase reference locality yet gain little in total execution performance. Our measurements suggest an allocator design that is both very fast and has good locality of reference.},
booktitle = {Proceedings of the ACM SIGPLAN 1993 Conference on Programming Language Design and Implementation},
pages = {177–186},
numpages = {10},
location = {Albuquerque, New Mexico, USA},
series = {PLDI '93}
}

@techreport{leijen2019mimalloc,
author = {Leijen, Daan and Zorn, Ben and de Moura, Leonardo},
title = {Mimalloc: Free List Sharding in Action},
institution = {Microsoft},
year = {2019},
month = {June},
abstract = {Modern memory allocators have to balance many simultaneous demands,
including performance, security, the presence of concurrency, and
application-specific demands depending on the context of their use. One
increasing use-case for allocators is as back-end implementations of
languages, such as Swift and Python, that use reference counting to
automatically deallocate objects. We present mimalloc, a memory allocator
that effectively balances these demands, shows significant performance
advantages over existing allocators, and is tailored to support languages
that rely on the memory allocator as a backend for reference counting.
Mimalloc combines several innovations to achieve this result. First, it
uses three page-local sharded free lists to increase locality, avoid
contention, and support a highly-tuned allocate and free fast path. These
free lists also support *temporal cadence*, which allows the allocator to
predictably leave the fast path for regular maintenance tasks such as
supporting deferred freeing, handling frees from non-local threads, etc.
While influenced by the allocation workload of the reference-counted Lean
and Koka programming language, we show that mimalloc has superior
performance to modern commercial memory allocators, including tcmalloc
and jemalloc, with speed improvements of 7% and 14%, respectively, on
redis, and consistently out performs over a wide range of sequential and
concurrent benchmarks. Allocators tailored to provide an efficient
runtime for reference-counting languages reduce the implementation burden
on developers and encourage the creation of innovative new language
designs.},
url = {https://www.microsoft.com/en-us/research/publication/mimalloc-free-list-sharding-in-action/},
number = {MSR-TR-2019-18},
}

@inproceedings{snmalloc,
author = {Li\'{e}tar, Paul and Butler, Theodore and Clebsch, Sylvan and Drossopoulou, Sophia and Franco, Juliana and Parkinson, Matthew J. and Shamis, Alex and Wintersteiger, Christoph M. and Chisnall, David},
title = {snmalloc: a message passing allocator},
year = {2019},
isbn = {9781450367226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3315573.3329980},
doi = {10.1145/3315573.3329980},
abstract = {snmalloc is an implementation of malloc aimed at workloads in which objects are typically deallocated by a different thread than the one that had allocated them. We use the term producer/consumer for such workloads. snmalloc uses a novel message passing scheme which returns deallocated objects to the originating allocator in batches without taking any locks. It also uses a novel bump pointer-free list data structure with which just 64-bits of meta-data are sufficient for each 64 KiB slab. On such producer/consumer benchmarks our approach performs better than existing allocators. Snmalloc is available at <a href="https://github.com/Microsoft/snmalloc">https://github.com/Microsoft/snmalloc</a>.},
booktitle = {Proceedings of the 2019 ACM SIGPLAN International Symposium on Memory Management},
pages = {122–135},
numpages = {14},
keywords = {message passing, Memory allocation},
location = {Phoenix, AZ, USA},
series = {ISMM 2019}
}

@inproceedings{haskell,
author = {Marlow, Simon and Harris, Tim and James, Roshan P. and Peyton Jones, Simon},
title = {Parallel generational-copying garbage collection with a block-structured heap},
year = {2008},
isbn = {9781605581347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1375634.1375637},
doi = {10.1145/1375634.1375637},
abstract = {We present a parallel generational-copying garbage collector implemented for the Glasgow Haskell Compiler. We use a block-structured memory allocator, which provides a natural granularity for dividing the work of GC between many threads, leading to a simple yet effective method for parallelising copying GC. The results are encouraging: we demonstrate wall-clock speedups of on average a factor of 2 in GC time on a commodity 4-core machine with no programmer intervention, compared to our best sequential GC.},
booktitle = {Proceedings of the 7th International Symposium on Memory Management},
pages = {11–20},
numpages = {10},
keywords = {parallel garbage collection},
location = {Tucson, AZ, USA},
series = {ISMM '08}
}

@inproceedings{ocaml-pm,
author = {Le Fessant, Fabrice and Maranget, Luc},
title = {Optimizing pattern matching},
year = {2001},
isbn = {1581134150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/507635.507641},
doi = {10.1145/507635.507641},
abstract = {We present improvements to the backtracking technique of pattern-matching compilation. Several optimizations are introduced, such as commutation of patterns, use of exhaustiveness information, and control flow optimization through the use of labeled static exceptions and context information. These optimizations have been integrated in the Objective-Caml compiler. They have shown good results in increasing the speed of pattern-matching intensive programs, without increasing final code size.},
booktitle = {Proceedings of the Sixth ACM SIGPLAN International Conference on Functional Programming},
pages = {26–37},
numpages = {12},
location = {Florence, Italy},
series = {ICFP '01}
}

@inproceedings{ocaml,
author = {Doligez, Damien and Gonthier, Georges},
title = {Portable, unobtrusive garbage collection for multiprocessor systems},
year = {1994},
isbn = {0897916360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/174675.174673},
doi = {10.1145/174675.174673},
abstract = {We describe and prove the correctness of a new concurrent mark-and-sweep garbage collection algorithm. This algorithm derives from the classical on-the-fly algorithm from Dijkstra et al. [9]. A distinguishing feature of our algorithm is that it supports multiprocessor environments where the registers of running processes are not readily accessible, without imposing any overhead on the elementary operations of loading a register or reading or initializing a field. Furthermore our collector never blocks running mutator processes except possibly on requests for free memory; in particular, updating a field or creating or marking or sweeping a heap object does not involve system-dependent synchronization primitives such as locks. We also provide support for process creation and deletion, and for managing an extensible heap of variable-sized objects.},
booktitle = {Proceedings of the 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {70–83},
numpages = {14},
location = {Portland, Oregon, USA},
series = {POPL '94}
}

@inproceedings{10.1145/800020.808261,
author = {Ungar, David},
title = {Generation Scavenging: A non-disruptive high performance storage reclamation algorithm},
year = {1984},
isbn = {0897911318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800020.808261},
doi = {10.1145/800020.808261},
abstract = {Many interactive computing environments provide automatic storage reclamation and virtual memory to ease the burden of managing storage. Unfortunately, many storage reclamation algorithms impede interaction with distracting pauses. Generation Scavenging is a reclamation algorithm that has no noticeable pauses, eliminates page faults for transient objects, compacts objects without resorting to indirection, and reclaims circular structures, in one third the time of traditional approaches.We have incorporated Generation Scavenging in Berkeley Smalltalk(BS), our Smalltalk-80 implementation, and instrumented it to obtain performance data. We are also designing a microprocessor with hardware support for Generation Scavenging.},
booktitle = {Proceedings of the First ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments},
pages = {157–167},
numpages = {11},
keywords = {Generation, Grabage collection, Personnel computer, Real time, Scavenge, Smalltalk, Virtual memory, Workstation},
series = {SDE 1}
}


@article{erlang-1,
author = {Ungar, David},
title = {Generation Scavenging: A non-disruptive high performance storage reclamation algorithm},
year = {1984},
issue_date = {May 1984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {0362-1340},
url = {https://doi.org/10.1145/390011.808261},
doi = {10.1145/390011.808261},
abstract = {Many interactive computing environments provide automatic storage reclamation and virtual memory to ease the burden of managing storage. Unfortunately, many storage reclamation algorithms impede interaction with distracting pauses. Generation Scavenging is a reclamation algorithm that has no noticeable pauses, eliminates page faults for transient objects, compacts objects without resorting to indirection, and reclaims circular structures, in one third the time of traditional approaches.We have incorporated Generation Scavenging in Berkeley Smalltalk(BS), our Smalltalk-80 implementation, and instrumented it to obtain performance data. We are also designing a microprocessor with hardware support for Generation Scavenging.},
journal = {SIGPLAN Not.},
month = {apr},
pages = {157–167},
numpages = {11},
keywords = {Generation, Grabage collection, Personnel computer, Real time, Scavenge, Smalltalk, Virtual memory, Workstation}
}

@article{erlang-2,
author = {Cheney, C. J.},
title = {A nonrecursive list compacting algorithm},
year = {1970},
issue_date = {Nov 1970},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/362790.362798},
doi = {10.1145/362790.362798},
abstract = {A simple nonrecursive list structure compacting scheme or garbage collector suitable for both compact and LISP-like list structures is presented. The algorithm avoids the need for recursion by using the partial structure as it is built up to keep track of those lists that have been copied.},
journal = {Commun. ACM},
month = {nov},
pages = {677–678},
numpages = {2},
keywords = {LISP, compact list, garbage collection, list compacting}
}

@inproceedings{perceus,
author = {Reinking, Alex and Xie, Ningning and de Moura, Leonardo and Leijen, Daan},
title = {Perceus: garbage free reference counting with reuse},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454032},
doi = {10.1145/3453483.3454032},
abstract = {We introduce Perceus, an algorithm for precise reference counting with reuse and specialization. Starting from a functional core language with explicit control-flow, Perceus emits precise reference counting instructions such that (cycle-free) programs are _garbage free_, where only live references are retained. This enables further optimizations, like reuse analysis that allows for guaranteed in-place updates at runtime. This in turn enables a novel programming paradigm that we call _functional but in-place_ (FBIP). Much like tail-call optimization enables writing loops with regular function calls, reuse analysis enables writing in-place mutating algorithms in a purely functional way. We give a novel formalization of reference counting in a linear resource calculus, and prove that Perceus is sound and garbage free. We show evidence that Perceus, as implemented in Koka, has good performance and is competitive with other state-of-the-art memory collectors.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {96–111},
numpages = {16},
keywords = {Reference Counting, Handlers, Algebraic Effects},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@article{frame-limited,
author = {Lorenzen, Anton and Leijen, Daan},
title = {Reference counting with frame limited reuse},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {ICFP},
url = {https://doi.org/10.1145/3547634},
doi = {10.1145/3547634},
abstract = {The recently introduced _Perceus_ algorithm can automatically insert  
reference count instructions such that the resulting (cycle-free) program is  
_garbage free_: objects are freed at the very moment they can no longer be  
referenced. An important extension is reuse analysis. This optimization pairs  
objects of known size with fresh allocations of the same size and tries to  
reuse the object in-place at runtime if it happens to be unique. Unfortunately,  
current implementations of reuse analysis are fragile with respect to small  
program transformations, or can cause an arbitrary increase in the peak heap  
usage. We present a novel _drop-guided_ reuse algorithm that is simpler and  
more robust than previous approaches. Moreover, we generalize the linear  
resource calculus to precisely characterize garbage-free and frame-limited  
evaluations. On each function call, a frame-limited evaluation may hold on to  
memory longer if the size is bounded by a constant factor. Using this framework  
we show that our drop-guided reuse _is_ frame-limited and find that an  
implementation of our new reuse approach in Koka can provide significant  
speedups.},
journal = {Proc. ACM Program. Lang.},
month = {aug},
articleno = {103},
numpages = {24},
keywords = {Frame Limited, Koka, Reference Counting, Reuse}
}

@inproceedings{lean-4,
  author = {Moura, Leonardo de
and Ullrich, Sebastian},
  editor = {Platzer, Andr{\'e}
and Sutcliffe, Geoff},
  title = {The Lean 4 Theorem Prover and Programming Language},
  booktitle = {Automated Deduction -- CADE 28},
  year = {2021},
  publisher = {Springer International Publishing},
  address = {Cham},
  pages = {625--635},
  isbn = {978-3-030-79876-5},
  documenturl = {https://leanprover.github.io/papers/lean4.pdf}
}

@inproceedings{lxr,
author = {Zhao, Wenyu and Blackburn, Stephen M. and McKinley, Kathryn S.},
title = {Low-latency, high-throughput garbage collection},
year = {2022},
isbn = {9781450392655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3519939.3523440},
doi = {10.1145/3519939.3523440},
abstract = {To achieve short pauses, state-of-the-art concurrent copying collectors such as C4, Shenandoah, and ZGC use substantially more CPU cycles and memory than simpler collectors. They suffer from design limitations: i) concurrent copying with inherently expensive read and write barriers, ii) scalability limitations due to tracing, and iii) immediacy limitations for mature objects that impose memory overheads.  

This paper takes a different approach to optimizing responsiveness and throughput. It uses the insight that regular, brief stop-the-world collections deliver sufficient responsiveness at greater efficiency than concurrent evacuation. It introduces LXR, where stop-the-world collections use reference counting (RC) and judicious copying. RC delivers scalability and immediacy, promptly reclaiming young and mature objects. RC, in a hierarchical Immix heap structure, reclaims most memory without any copying. Occasional concurrent tracing identifies cyclic garbage. LXR introduces: i) RC remembered sets for judicious copying of mature objects; ii) a novel low-overhead write barrier that combines coalescing reference counting, concurrent tracing, and remembered set maintenance; iii) object reclamation while performing a concurrent trace; iv) lazy processing of decrements; and v) novel survival rate triggers that modulate pause durations.  

LXR combines excellent responsiveness and throughput, improving over production collectors. On the widely-used Lucene search engine in a tight heap, LXR delivers 7.8\texttimes{} better throughput and 10\texttimes{} better 99.99\% tail latency than Shenandoah. On 17 diverse modern workloads in a moderate heap, LXR outperforms OpenJDK’s default G1 on throughput by 4\% and Shenandoah by 43\%.},
booktitle = {Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {76–91},
numpages = {16},
keywords = {Garbage collection, Reference counting},
location = {San Diego, CA, USA},
series = {PLDI 2022}
}

@inproceedings{immix,
author = {Blackburn, Stephen M. and McKinley, Kathryn S.},
title = {Immix: a mark-region garbage collector with space efficiency, fast collection, and mutator performance},
year = {2008},
isbn = {9781595938602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1375581.1375586},
doi = {10.1145/1375581.1375586},
abstract = {Programmers are increasingly choosing managed languages for modern applications, which tend to allocate many short-to-medium lived small objects. The garbage collector therefore directly determines program performance by making a classic space-time tradeoff that seeks to provide space efficiency, fast reclamation, and mutator performance. The three canonical tracing garbage collectors: semi-space, mark-sweep, and mark-compact each sacrifice one objective. This paper describes a collector family, called mark-region, and introduces opportunistic defragmentation, which mixes copying and marking in a single pass. Combining both, we implement immix, a novel high performance garbage collector that achieves all three performance objectives. The key insight is to allocate and reclaim memory in contiguous regions, at a coarse block grain when possible and otherwise in groups of finer grain lines. We show that immix outperforms existing canonical algorithms, improving total application performance by 7 to 25\% on average across 20 benchmarks. As the mature space in a generational collector, immix matches or beats a highly tuned generational collector, e.g. it improves jbb2000 by 5\%. These innovations and the identification of a new family of collectors open new opportunities for garbage collector design.},
booktitle = {Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {22–32},
numpages = {11},
keywords = {sweep-to-region, sweep-to-free-list, semi-space, mark-sweep, mark-region, locality, immix, free-list, fragmentation, compaction},
location = {Tucson, AZ, USA},
series = {PLDI '08}
}

@article{hoas,
author = {Pfenning, F. and Elliott, C.},
title = {Higher-order abstract syntax},
year = {1988},
issue_date = {July 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {7},
issn = {0362-1340},
url = {https://doi.org/10.1145/960116.54010},
doi = {10.1145/960116.54010},
abstract = {We describe motivation, design, use, and implementation of higher-order abstract syntax as a central representation for programs, formulas, rules, and other syntactic objects in program manipulation and other formal systems where matching and substitution or unification are central operations. Higher-order abstract syntax incorporates name binding information in a uniform and language generic way. Thus it acts as a powerful link integrating diverse tools in such formal environments. We have implemented higher-order abstract syntax, a supporting matching and unification algorithm, and some clients in Common Lisp in the framework of the Ergo project at Carnegie Mellon University.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {199–208},
numpages = {10}
}

@inproceedings{linear,
  title={Linear Types can Change the World!},
  author={Philip Wadler},
  booktitle={Programming Concepts and Methods},
  year={1990},
  url={https://api.semanticscholar.org/CorpusID:58535510}
}

@Book{hottbook,
  author =    {The {Univalent Foundations Program}},
  title =     {Homotopy Type Theory: Univalent Foundations of Mathematics},
  publisher = {\url{https://homotopytypetheory.org/book}},
  address =   {Institute for Advanced Study},
  year =      2013}

@inproceedings{substructural,

  author={Pfenning, Frank and Simmons, Robert J.},

  booktitle={2009 24th Annual IEEE Symposium on Logic In Computer Science}, 

  title={Substructural Operational Semantics as Ordered Logic Programming}, 

  year={2009},

  volume={},

  number={},

  pages={101-110},

  keywords={Logic programming;Computer languages;Computer science;Parallel processing;Greedy algorithms;Calculus},

  doi={10.1109/LICS.2009.8}}

@article{fp2,
author = {Lorenzen, Anton and Leijen, Daan and Swierstra, Wouter},
title = {FP²: Fully in-Place Functional Programming},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {ICFP},
url = {https://doi.org/10.1145/3607840},
doi = {10.1145/3607840},
abstract = {As functional programmers we always face a dilemma: should we write purely  
functional code, or sacrifice purity for efficiency and resort to in-place  
updates? This paper identifies precisely when we can have the best of both  
worlds: a wide class of purely functional programs can be executed safely using  
in-place updates without requiring allocation, provided their arguments are not  
shared elsewhere.  

We describe a linear _fully in-place_ (FIP) calculus where we prove that we can  
always execute such functions in a way that requires no (de)allocation and uses  
constant stack space. Of course, such a calculus is only relevant if we can  
express interesting algorithms; we provide numerous examples of in-place  
functions on datastructures such as splay trees or finger trees, together with  
in-place versions of merge sort and quick sort.  

We also show how we can generically derive a map function over _any_ polynomial  
data type that is fully in-place. Finally, we have implemented the rules of the  
FIP calculus in the Koka language. Using the Perceus reference counting garbage  
collection, this implementation dynamically executes FIP functions in-place  
whenever possible.},
journal = {Proc. ACM Program. Lang.},
month = {aug},
articleno = {198},
numpages = {30},
keywords = {FBIP, Tail Recursion Modulo Cons}
}
  


  




  




  

